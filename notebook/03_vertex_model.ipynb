{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from reformer_pytorch import Reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7003 1088\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(base_dir, \"data\", \"original\")\n",
    "train_files = glob.glob(os.path.join(data_dir, \"train\", \"*\", \"*.obj\"))\n",
    "valid_files = glob.glob(os.path.join(data_dir, \"val\", \"*\", \"*.obj\"))\n",
    "print(len(train_files), len(valid_files))\n",
    "\n",
    "src_dir = os.path.join(base_dir, \"src\")\n",
    "sys.path.append(os.path.join(src_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_polygen import load_pipeline\n",
    "from tokenizers import DecodeVertexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 3]) 160\n",
      "============================================================\n",
      "torch.Size([62, 3]) 45\n",
      "============================================================\n",
      "torch.Size([64, 3]) 601\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "v_batch, f_batch = [], []\n",
    "for i in range(3):\n",
    "    vs, _, fs = load_pipeline(train_files[i])\n",
    "    \n",
    "    vs = torch.tensor(vs)\n",
    "    fs = [torch.tensor(f) for f in fs]\n",
    "    \n",
    "    v_batch.append(vs)\n",
    "    f_batch.append(fs)\n",
    "    print(vs.shape, len(fs))\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tokenizer = DecodeVertexTokenizer(max_seq_len=2592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value_tokens': tensor([[  0, 169, 124,  ...,   2,   2,   2],\n",
       "         [  0, 167, 166,  ...,   2,   2,   2],\n",
       "         [  0, 167, 167,  ...,   2,   2,   2]]),\n",
       " 'target_tokens': tensor([[169, 124, 169,  ...,   2,   2,   2],\n",
       "         [167, 166, 167,  ...,   2,   2,   2],\n",
       "         [167, 167, 130,  ...,   2,   2,   2]]),\n",
       " 'coord_type_tokens': tensor([[0, 1, 2,  ..., 0, 0, 0],\n",
       "         [0, 1, 2,  ..., 0, 0, 0],\n",
       "         [0, 1, 2,  ..., 0, 0, 0]]),\n",
       " 'position_tokens': tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'padding_mask': tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = dec_tokenizer.tokenize(v_batch)\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexDecoderEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim=256,\n",
    "                 vocab_value=259, pad_idx_value=2, \n",
    "                 vocab_coord_type=4, pad_idx_coord_type=0,\n",
    "                 vocab_position=1000, pad_idx_position=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.value_embed = nn.Embedding(\n",
    "            vocab_value, embed_dim, padding_idx=pad_idx_value\n",
    "        )\n",
    "        self.coord_type_embed = nn.Embedding(\n",
    "            vocab_coord_type, embed_dim, padding_idx=pad_idx_coord_type\n",
    "        )\n",
    "        self.position_embed = nn.Embedding(\n",
    "            vocab_position, embed_dim, padding_idx=pad_idx_position\n",
    "        )\n",
    "        \n",
    "        self.embed_scaler = math.sqrt(embed_dim)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        \"\"\"get embedding for vertex model.\n",
    "        \n",
    "        Args\n",
    "            tokens [dict]: tokenized vertex info.\n",
    "                `value_tokens` [torch.tensor]:\n",
    "                        padded (batch, length)-shape long tensor\n",
    "                        with coord value from 0 to 2^n(bit).\n",
    "                `coord_type_tokens` [torch.tensor]:\n",
    "                        padded (batch, length) shape long tensor implies x or y or z.\n",
    "                `position_tokens` [torch.tensor]:\n",
    "                        padded (batch, length) shape long tensor\n",
    "                        representing coord position (NOT sequence position).\n",
    "        \n",
    "        Returns\n",
    "            embed [torch.tensor]: (batch, length, embed) shape tensor after embedding.\n",
    "                        \n",
    "        \"\"\"\n",
    "        \n",
    "        embed = self.value_embed(tokens[\"value_tokens\"])\n",
    "        embed = embed + self.coord_type_embed(tokens[\"coord_type_tokens\"])\n",
    "        embed = embed + self.position_embed(tokens[\"position_tokens\"])\n",
    "        embed = embed * self.embed_scaler\n",
    "        \n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = VertexDecoderEmbedding(embed_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value_tokens': tensor([[  0, 169, 124,  ...,   2,   2,   2],\n",
       "         [  0, 167, 166,  ...,   2,   2,   2],\n",
       "         [  0, 167, 167,  ...,   2,   2,   2]]),\n",
       " 'target_tokens': tensor([[169, 124, 169,  ...,   2,   2,   2],\n",
       "         [167, 166, 167,  ...,   2,   2,   2],\n",
       "         [167, 167, 130,  ...,   2,   2,   2]]),\n",
       " 'coord_type_tokens': tensor([[0, 1, 2,  ..., 0, 0, 0],\n",
       "         [0, 1, 2,  ..., 0, 0, 0],\n",
       "         [0, 1, 2,  ..., 0, 0, 0]]),\n",
       " 'position_tokens': tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'padding_mask': tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2592]) torch.Size([3, 2592]) torch.Size([3, 2592])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    input_tokens[\"value_tokens\"].shape,\n",
    "    input_tokens[\"coord_type_tokens\"].shape,\n",
    "    input_tokens[\"position_tokens\"].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2592, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = embed(input_tokens)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformer = Reformer(dim=128, depth=1, max_seq_len=8192, bucket_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2592, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = reformer(emb)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \n",
    "    def write_to_json(self, out_path):\n",
    "        with open(out_path, \"w\") as fw:\n",
    "            json.dump(self.config, fw, indent=4)\n",
    "            \n",
    "    def load_from_json(self, file_path):\n",
    "        with open(file_path) as fr:\n",
    "            self.config = json.load(fr)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self.config[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexPolyGenConfig(Config):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_dim=256, \n",
    "                 max_seq_len=2400, \n",
    "                 tokenizer__bos_id=0,\n",
    "                 tokenizer__eos_id=1,\n",
    "                 tokenizer__pad_id=2,\n",
    "                 embedding__vocab_value=256 + 3, \n",
    "                 embedding__vocab_coord_type=4, \n",
    "                 embedding__vocab_position=1000,\n",
    "                 embedding__pad_idx_value=2,\n",
    "                 embedding__pad_idx_coord_type=0,\n",
    "                 embedding__pad_idx_position=0,\n",
    "                 reformer__depth=12,\n",
    "                 reformer__heads=8,\n",
    "                 reformer__n_hashes=8,\n",
    "                 reformer__bucket_size=48,\n",
    "                 reformer__causal=True,\n",
    "                 reformer__lsh_dropout=0.2, \n",
    "                 reformer__ff_dropout=0.2,\n",
    "                 reformer__post_attn_dropout=0.2,\n",
    "                 reformer__ff_mult=4):\n",
    "        \n",
    "        # tokenizer config\n",
    "        tokenizer_config = {\n",
    "            \"bos_id\": tokenizer__bos_id,\n",
    "            \"eos_id\": tokenizer__eos_id,\n",
    "            \"pad_id\": tokenizer__pad_id,\n",
    "            \"max_seq_len\": max_seq_len,\n",
    "        }\n",
    "        \n",
    "        # embedding config\n",
    "        embedding_config = {\n",
    "            \"vocab_value\": embedding__vocab_value,\n",
    "            \"vocab_coord_type\": embedding__vocab_coord_type,\n",
    "            \"vocab_position\": embedding__vocab_position,\n",
    "            \"pad_idx_value\": embedding__pad_idx_value,\n",
    "            \"pad_idx_coord_type\": embedding__pad_idx_coord_type,\n",
    "            \"pad_idx_position\": embedding__pad_idx_position,\n",
    "            \"embed_dim\": embed_dim,\n",
    "        }\n",
    "        \n",
    "        # reformer info\n",
    "        reformer_config = {\n",
    "            \"dim\": embed_dim,\n",
    "            \"depth\": reformer__depth,\n",
    "            \"max_seq_len\": max_seq_len,\n",
    "            \"heads\": reformer__heads,\n",
    "            \"bucket_size\": reformer__bucket_size,\n",
    "            \"n_hashes\": reformer__n_hashes,\n",
    "            \"causal\": reformer__causal,\n",
    "            \"lsh_dropout\": reformer__lsh_dropout, \n",
    "            \"ff_dropout\": reformer__ff_dropout,\n",
    "            \"post_attn_dropout\": reformer__post_attn_dropout,\n",
    "            \"ff_mult\": reformer__ff_mult,\n",
    "        }\n",
    "        \n",
    "        self.config = {\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"max_seq_len\": max_seq_len,\n",
    "            \"tokenizer\": tokenizer_config,\n",
    "            \"embedding\": embedding_config,\n",
    "            \"reformer\": reformer_config,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def accuracy(y_pred, y_true, ignore_label=None, device=None):\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "\n",
    "    if ignore_label:\n",
    "        normalizer = torch.sum(y_true!=ignore_label)\n",
    "        ignore_mask = torch.where(\n",
    "            y_true == ignore_label,\n",
    "            torch.zeros_like(y_true, device=device),\n",
    "            torch.ones_like(y_true, device=device)\n",
    "        ).type(torch.float32)\n",
    "    else:\n",
    "        normalizer = y_true.shape[0]\n",
    "        ignore_mask = torch.ones_like(y_true, device=device).type(torch.float32)\n",
    "\n",
    "    acc = (y_pred.reshape(-1)==y_true.reshape(-1)).type(torch.float32)\n",
    "    acc = torch.sum(acc*ignore_mask)\n",
    "    return acc / normalizer\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "    if type(m) == nn.Embedding:\n",
    "        nn.init.uniform_(m.weight, -0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexPolyGen(nn.Module):\n",
    "    \n",
    "    \"\"\"Vertex model in PolyGen.\n",
    "    this model learn/predict vertices like OpenAI-GPT.\n",
    "    UNLIKE the paper, this model is only for unconditional generation.\n",
    "    \n",
    "    Args\n",
    "        model_config [Config]:\n",
    "                hyper parameters. see VertexPolyGenConfig class for details. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = DecodeVertexTokenizer(**model_config[\"tokenizer\"])\n",
    "        self.embedding = VertexDecoderEmbedding(**model_config[\"embedding\"])\n",
    "        self.reformer = Reformer(**model_config[\"reformer\"])\n",
    "        self.layernorm = nn.LayerNorm(model_config[\"embed_dim\"])\n",
    "        self.loss_func = nn.CrossEntropyLoss(ignore_index=model_config[\"tokenizer\"][\"pad_id\"])\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def forward(self, tokens, device=None):\n",
    "        \n",
    "        \"\"\"forward function which can be used for both train/predict.\n",
    "        \n",
    "        Args\n",
    "            tokens [dict]: tokenized vertex info.\n",
    "                `value_tokens` [torch.tensor]:\n",
    "                        padded (batch, length)-shape long tensor\n",
    "                        with coord value from 0 to 2^n(bit).\n",
    "                `coord_type_tokens` [torch.tensor]:\n",
    "                        padded (batch, length) shape long tensor implies x or y or z.\n",
    "                `position_tokens` [torch.tensor]:\n",
    "                        padded (batch, length) shape long tensor\n",
    "                        representing coord position (NOT sequence position).\n",
    "                `padding_mask` [torch.tensor]:\n",
    "                        (batch, length) shape mask implies <pad> tokens.\n",
    "            device [torch.device]: gpu or not gpu, that's the problem.\n",
    "            \n",
    "        \n",
    "        Returns\n",
    "            hs [torch.tensor]:\n",
    "                    hidden states from transformer(reformer) model.\n",
    "                    this takes (batch, length, embed) shape.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        hs = self.embedding(tokens)\n",
    "        hs = self.reformer(\n",
    "            hs, input_mask=tokens[\"padding_mask\"]\n",
    "        )\n",
    "        hs = self.layernorm(hs)\n",
    "        \n",
    "        return hs\n",
    "        \n",
    "        \n",
    "    def __call__(self, inputs, device=None):\n",
    "        \n",
    "        \"\"\"Calculate loss while training.\n",
    "        \n",
    "        Args\n",
    "            inputs [dict]: dict containing batched inputs.\n",
    "                `vertices` [list(torch.tensor)]:\n",
    "                        variable-length-list of \n",
    "                        (length, 3) shaped tensor of quantized-vertices.\n",
    "            device [torch.device]: gpu or not gpu, that's the problem.\n",
    "                \n",
    "        Returns\n",
    "            outputs [dict]: dict containing calculated variables.\n",
    "                `loss` [torch.tensor]:\n",
    "                        calculated scalar-shape loss with backprop info.\n",
    "                `accuracy` [torch.tensor]:\n",
    "                        calculated scalar-shape accuracy.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(inputs[\"vertices\"])\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        \n",
    "        hs = self.forward(tokens, device=device)\n",
    "        \n",
    "        hs = F.linear(hs, self.embedding.value_embed.weight)\n",
    "        BATCH, LENGTH, EMBED = hs.shape\n",
    "        hs = hs.reshape(BATCH*LENGTH, EMBED)\n",
    "        targets = tokens[\"target_tokens\"].reshape(BATCH*LENGTH,)\n",
    "        \n",
    "        acc = accuracy(\n",
    "            hs, targets, ignore_label=self.tokenizer.pad_id, device=device\n",
    "        )\n",
    "        loss = self.loss_func(hs, targets)\n",
    "        \n",
    "        outputs = {\n",
    "            \"accuracy\": acc,\n",
    "            \"perplexity\": torch.exp(loss),\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, max_seq_len=2400, device=None):\n",
    "        \"\"\"predict function\n",
    "        \n",
    "        Args\n",
    "            max_seq_len[int]: max sequence length to predict.\n",
    "            device [torch.device]: gpu or not gpu, that's the problem.\n",
    "            \n",
    "        Return\n",
    "            preds [torch.tensor]: predicted (length, ) shape tensor.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        tokenizer = self.tokenizer\n",
    "        special_tokens = tokenizer.special_tokens\n",
    "        \n",
    "        tokens = tokenizer.get_pred_start()\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        preds = []\n",
    "        pred_idx = 0\n",
    "        \n",
    "        while (pred_idx <= max_seq_len-1)\\\n",
    "        and ((len(preds) == 0) or (preds[-1] != special_tokens[\"eos\"]-len(special_tokens))):\n",
    "            \n",
    "            if pred_idx >= 1:\n",
    "                tokens = tokenizer.tokenize([torch.stack(preds)])\n",
    "                tokens[\"value_tokens\"][:, pred_idx+1] = special_tokens[\"pad\"]\n",
    "                tokens[\"padding_mask\"][:, pred_idx+1] = True\n",
    "            \n",
    "            hs = self.forward(tokens, device=device)\n",
    "\n",
    "            hs = F.linear(hs[:, pred_idx], self.embedding.value_embed.weight)\n",
    "            pred = hs.argmax(dim=1) - len(special_tokens)\n",
    "            preds.append(pred[0])\n",
    "            pred_idx += 1\n",
    "            \n",
    "        preds = torch.stack(preds) + len(special_tokens)\n",
    "        preds = self.tokenizer.detokenize([preds])[0]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VertexPolyGenConfig(\n",
    "    embed_dim=128, reformer__depth=6, \n",
    "    reformer__lsh_dropout=0., reformer__ff_dropout=0.,\n",
    "    reformer__post_attn_dropout=0.\n",
    ")\n",
    "model = VertexPolyGen(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 3])\n",
      "torch.Size([62, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"vertices\": v_batch[:2],\n",
    "}\n",
    "for b in inputs[\"vertices\"]:\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\tloss: 5.57170\tperp: 262.881\tacc: 0.05500\n",
      "iteration: 10\tloss: 4.82134\tperp: 129.929\tacc: 0.14925\n",
      "iteration: 20\tloss: 4.07001\tperp: 59.521\tacc: 0.28187\n",
      "iteration: 30\tloss: 3.47319\tperp: 32.687\tacc: 0.42400\n",
      "iteration: 40\tloss: 2.89775\tperp: 18.384\tacc: 0.59175\n",
      "iteration: 50\tloss: 2.31088\tperp: 10.229\tacc: 0.76762\n",
      "iteration: 60\tloss: 1.73632\tperp: 5.747\tacc: 0.89712\n",
      "iteration: 70\tloss: 1.23784\tperp: 3.476\tacc: 0.96250\n",
      "iteration: 80\tloss: 0.85495\tperp: 2.361\tacc: 0.98550\n",
      "iteration: 90\tloss: 0.59418\tperp: 1.815\tacc: 0.99587\n",
      "iteration: 100\tloss: 0.42693\tperp: 1.534\tacc: 0.99625\n",
      "iteration: 110\tloss: 0.32102\tperp: 1.379\tacc: 0.99625\n",
      "iteration: 120\tloss: 0.25241\tperp: 1.287\tacc: 0.99625\n",
      "iteration: 130\tloss: 0.20504\tperp: 1.228\tacc: 0.99625\n",
      "iteration: 140\tloss: 0.17135\tperp: 1.187\tacc: 0.99625\n",
      "iteration: 150\tloss: 0.14645\tperp: 1.158\tacc: 0.99625\n",
      "iteration: 160\tloss: 0.12735\tperp: 1.136\tacc: 0.99625\n",
      "iteration: 170\tloss: 0.11230\tperp: 1.119\tacc: 0.99625\n",
      "iteration: 180\tloss: 0.10016\tperp: 1.105\tacc: 0.99625\n",
      "iteration: 190\tloss: 0.09027\tperp: 1.094\tacc: 0.99625\n",
      "iteration: 200\tloss: 0.08188\tperp: 1.085\tacc: 0.99625\n",
      "iteration: 210\tloss: 0.07482\tperp: 1.078\tacc: 0.99625\n",
      "iteration: 220\tloss: 0.06877\tperp: 1.071\tacc: 0.99625\n",
      "iteration: 230\tloss: 0.06370\tperp: 1.066\tacc: 0.99625\n",
      "iteration: 240\tloss: 0.05911\tperp: 1.061\tacc: 0.99625\n",
      "iteration: 250\tloss: 0.05505\tperp: 1.057\tacc: 0.99625\n",
      "iteration: 260\tloss: 0.05150\tperp: 1.053\tacc: 0.99625\n",
      "iteration: 270\tloss: 0.04836\tperp: 1.050\tacc: 0.99625\n",
      "iteration: 280\tloss: 0.04555\tperp: 1.047\tacc: 0.99637\n",
      "iteration: 290\tloss: 0.04301\tperp: 1.044\tacc: 0.99625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "epoch_num = 300\n",
    "model.train()\n",
    "losses = []\n",
    "accs = []\n",
    "perps = []\n",
    "\n",
    "for i in range(epoch_num):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    loss = outputs[\"loss\"]\n",
    "    acc = outputs[\"accuracy\"]\n",
    "    perp = outputs[\"perplexity\"]\n",
    "    losses.append(loss.item())\n",
    "    accs.append(acc.item())\n",
    "    perps.append(perp.item())\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        ave_loss = np.mean(losses[-10:])\n",
    "        ave_acc = np.mean(accs[-10:])\n",
    "        ave_perp = np.mean(perps[-10:])\n",
    "        print(\"iteration: {}\\tloss: {:.5f}\\tperp: {:.3f}\\tacc: {:.5f}\".format(\n",
    "            i, ave_loss, ave_perp, ave_acc))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([164, 163, 164, 164, 163,  90, 164, 154, 164, 164, 154,  90, 163, 154,\n",
       "        164, 163, 154, 163, 163, 154,  91, 163,  91, 163, 163,  91,  91, 162,\n",
       "        163, 162, 162, 163,  92, 162,  92, 162, 162,  92,  92, 162,  91, 162,\n",
       "        162,  91,  92, 144, 153,  92, 144, 153,  91, 144, 146,  92, 144, 146,\n",
       "         91, 138, 153, 163, 138, 153, 162, 138, 146, 163, 138, 146, 162, 133,\n",
       "        153,  92, 133, 153,  91, 133, 146,  92, 133, 146,  91, 128, 154,  92,\n",
       "        128, 154,  91, 128, 146,  92, 128, 146,  91, 125, 153, 163, 125, 153,\n",
       "        162, 125, 146, 163, 125, 146, 162, 121, 153, 163, 121, 153, 162, 121,\n",
       "        146, 163, 121, 146, 162, 117, 154,  92, 117, 154,  91, 117, 146,  92,\n",
       "        117, 146,  91, 111, 153, 163, 111, 153, 162, 111, 146, 163, 111, 146,\n",
       "        162,  92, 163, 162,  92, 163,  92,  92,  92, 162,  92,  92,  92,  92,\n",
       "         91, 162,  92,  91,  92,  91, 154, 163,  91, 154,  91,  91, 154,  90,\n",
       "         91,  91, 163,  91,  91,  91,  90, 163, 164,  90, 163,  90,  90, 154,\n",
       "        164,  90, 154,  90])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model.predict(max_seq_len=2400)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([166, 121, 166, 166, 121,  88, 166, 108, 166, 166, 108,  88, 165, 106,\n",
       "        165, 165, 106,  89, 165, 104, 165, 165, 104,  89, 165, 103, 165, 165,\n",
       "        103,  89, 164, 121, 164, 164, 121,  90, 164, 108, 164, 164, 108,  90,\n",
       "        164, 106, 164, 164, 106,  90, 164, 105, 164, 164, 105,  90, 164, 101,\n",
       "        164, 164, 101,  90, 163, 103, 163, 163, 103,  91, 163, 102, 163, 163,\n",
       "        102,  91, 163,  99, 163, 163,  99,  91, 162, 100, 162, 162, 100,  92,\n",
       "        162,  98, 162, 162,  98,  92, 161,  99, 161, 161,  99,  93, 160,  97,\n",
       "        160, 160,  97,  94, 159,  98, 159, 159,  98,  95, 159,  96, 159, 159,\n",
       "         96,  95, 158,  97, 158, 158,  97,  96, 157,  96, 157, 157,  96,  97,\n",
       "        157,  95, 157, 157,  95,  97, 155,  96, 155, 155,  96,  99, 155,  94,\n",
       "        155, 155,  94,  99, 153,  95, 153, 153,  95, 101, 153,  94, 153, 153,\n",
       "         94, 101, 152,  95, 152, 152,  95, 102, 152,  94, 152, 152,  94, 102,\n",
       "        131, 160, 161, 131, 160, 160, 131, 160, 159, 131, 160,  95, 131, 160,\n",
       "         94, 131, 160,  93, 131, 159, 163, 131, 159, 162, 131, 159, 161, 131,\n",
       "        159, 160, 131, 159, 159, 131, 159,  95, 131, 159,  94, 131, 159,  93,\n",
       "        131, 159,  92, 131, 159,  91, 131, 158, 164, 131, 158, 162, 131, 158,\n",
       "         92, 131, 158,  90, 131, 157, 165, 131, 157, 164, 131, 157, 163, 131,\n",
       "        157,  91, 131, 157,  90, 131, 157,  89, 131, 156, 165, 131, 156, 164,\n",
       "        131, 156,  90, 131, 156,  89, 131, 155, 165, 131, 155, 164, 131, 155,\n",
       "         90, 131, 155,  89, 131, 154, 166, 131, 154, 164, 131, 154,  90, 131,\n",
       "        154,  88, 131, 153, 166, 131, 153, 164, 131, 153,  90, 131, 153,  88,\n",
       "        131, 121, 166, 131, 121, 164, 131, 121,  90, 131, 121,  88, 123, 160,\n",
       "        161, 123, 160, 160, 123, 160, 159, 123, 160,  95, 123, 160,  94, 123,\n",
       "        160,  93, 123, 159, 163, 123, 159, 162, 123, 159, 161, 123, 159, 160,\n",
       "        123, 159, 159, 123, 159,  95, 123, 159,  94, 123, 159,  93, 123, 159,\n",
       "         92, 123, 159,  91, 123, 158, 164, 123, 158, 162, 123, 158,  92, 123,\n",
       "        158,  90, 123, 157, 165, 123, 157, 164, 123, 157, 163, 123, 157,  91,\n",
       "        123, 157,  90, 123, 157,  89, 123, 156, 165, 123, 156, 164, 123, 156,\n",
       "         90, 123, 156,  89, 123, 155, 165, 123, 155, 164, 123, 155,  90, 123,\n",
       "        155,  89, 123, 154, 166, 123, 154, 164, 123, 154,  90, 123, 154,  88,\n",
       "        123, 153, 166, 123, 153, 164, 123, 153,  90, 123, 153,  88, 123, 121,\n",
       "        166, 123, 121, 164, 123, 121,  90, 123, 121,  88, 102,  95, 152, 102,\n",
       "         95, 102, 102,  94, 152, 102,  94, 102, 101,  95, 153, 101,  95, 101,\n",
       "        101,  94, 153, 101,  94, 101,  99,  96, 155,  99,  96,  99,  99,  94,\n",
       "        155,  99,  94,  99,  97,  96, 157,  97,  96,  97,  97,  95, 157,  97,\n",
       "         95,  97,  96,  97, 158,  96,  97,  96,  95,  98, 159,  95,  98,  95,\n",
       "         95,  96, 159,  95,  96,  95,  94,  97, 160,  94,  97,  94,  93,  99,\n",
       "        161,  93,  99,  93,  92, 100, 162,  92, 100,  92,  92,  98, 162,  92,\n",
       "         98,  92,  91, 103, 163,  91, 103,  91,  91, 102, 163,  91, 102,  91,\n",
       "         91,  99, 163,  91,  99,  91,  90, 121, 164,  90, 121,  90,  90, 108,\n",
       "        164,  90, 108,  90,  90, 106, 164,  90, 106,  90,  90, 105, 164,  90,\n",
       "        105,  90,  90, 101, 164,  90, 101,  90,  89, 106, 165,  89, 106,  89,\n",
       "         89, 104, 165,  89, 104,  89,  89, 103, 165,  89, 103,  89,  88, 121,\n",
       "        166,  88, 121,  88,  88, 108, 166,  88, 108,  88], dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = inputs[\"vertices\"][0].reshape(-1, )\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([612]), torch.Size([612]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.shape, pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8644)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (true == pred).sum() / len(true)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([186])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = inputs[\"vertices\"][1].reshape(-1, )\n",
    "true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../results/models/vertex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
