{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from reformer_pytorch import Reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7003 1088\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(base_dir, \"data\", \"original\")\n",
    "train_files = glob.glob(os.path.join(data_dir, \"train\", \"*\", \"*.obj\"))\n",
    "valid_files = glob.glob(os.path.join(data_dir, \"val\", \"*\", \"*.obj\"))\n",
    "print(len(train_files), len(valid_files))\n",
    "\n",
    "src_dir = os.path.join(base_dir, \"src\")\n",
    "sys.path.append(os.path.join(src_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_polygen import load_pipeline\n",
    "from tokenizers import EncodeVertexTokenizer, FaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 3]) 160\n",
      "============================================================\n",
      "torch.Size([62, 3]) 45\n",
      "============================================================\n",
      "torch.Size([64, 3]) 601\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "v_batch, f_batch = [], []\n",
    "for i in range(3):\n",
    "    vs, _, fs = load_pipeline(train_files[i])\n",
    "    \n",
    "    vs = torch.tensor(vs)\n",
    "    fs = [torch.tensor(f) for f in fs]\n",
    "    \n",
    "    v_batch.append(vs)\n",
    "    f_batch.append(fs)\n",
    "    print(vs.shape, len(fs))\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer = EncodeVertexTokenizer(max_seq_len=2592)\n",
    "dec_tokenizer = FaceTokenizer(max_seq_len=3936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value_tokens': tensor([[167, 122, 167,  ...,   0,   0,   0],\n",
       "         [165, 164, 165,  ...,   0,   0,   0],\n",
       "         [165, 165, 128,  ...,   0,   0,   0]]),\n",
       " 'coord_type_tokens': tensor([[1, 2, 3,  ..., 0, 0, 0],\n",
       "         [1, 2, 3,  ..., 0, 0, 0],\n",
       "         [1, 2, 3,  ..., 0, 0, 0]]),\n",
       " 'position_tokens': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'padding_mask': tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = enc_tokenizer.tokenize(v_batch)\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_tokens torch.Size([3, 2592])\n",
      "coord_type_tokens torch.Size([3, 2592])\n",
      "position_tokens torch.Size([3, 2592])\n",
      "padding_mask torch.Size([3, 2592])\n"
     ]
    }
   ],
   "source": [
    "for k, v in src_tokens.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEncoderEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim=256,\n",
    "                 vocab_value=259, pad_idx_value=2, \n",
    "                 vocab_coord_type=4, pad_idx_coord_type=0,\n",
    "                 vocab_position=1000, pad_idx_position=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.value_embed = nn.Embedding(\n",
    "            vocab_value, embed_dim, padding_idx=pad_idx_value\n",
    "        )\n",
    "        self.coord_type_embed = nn.Embedding(\n",
    "            vocab_coord_type, embed_dim, padding_idx=pad_idx_coord_type\n",
    "        )\n",
    "        self.position_embed = nn.Embedding(\n",
    "            vocab_position, embed_dim, padding_idx=pad_idx_position\n",
    "        )\n",
    "        \n",
    "        self.embed_scaler = math.sqrt(embed_dim)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        \"\"\"get embedding for Face Encoder.\n",
    "        \n",
    "        Args\n",
    "            tokens [dict]: tokenized vertex info.\n",
    "                `value_tokens` [torch.tensor]:\n",
    "                        padded (batch, length) shape long tensor\n",
    "                        with coord value from 0 to 2^n(bit).\n",
    "                `coord_type_tokens` [torch.tensor]:\n",
    "                        padded (batch, length) shape long tensor implies x or y or z.\n",
    "                `position_tokens` [torch.tensor]:\n",
    "                        padded (batch, length) shape long tensor\n",
    "                        representing coord position (NOT sequence position).\n",
    "        \n",
    "        Returns\n",
    "            embed [torch.tensor]: (batch, length, embed) shape tensor after embedding.\n",
    "                        \n",
    "        \"\"\"\n",
    "              \n",
    "        embed = self.value_embed(tokens[\"value_tokens\"])\n",
    "        embed = embed + self.coord_type_embed(tokens[\"coord_type_tokens\"])\n",
    "        embed = embed + self.position_embed(tokens[\"position_tokens\"])\n",
    "        embed = embed * self.embed_scaler\n",
    "        \n",
    "        embed = embed[:, :-1]\n",
    "        embed = torch.cat([\n",
    "            e.sum(dim=1).unsqueeze(dim=1) for e in embed.split(3, dim=1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        return embed\n",
    "    \n",
    "    def forward_original(self, tokens):\n",
    "        # original PolyGen embedding did something like this (no position info?).\n",
    "        embed = self.value_embed(tokens[\"value_tokens\"]) * self.embed_scaler\n",
    "        embed = torch.cat([\n",
    "            e.sum(dim=1).unsqueeze(dim=1) for e in embed[:, :-1].split(3, dim=1)\n",
    "        ], dim=1)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embed = FaceEncoderEmbedding(embed_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2592]) torch.Size([3, 2592]) torch.Size([3, 2592]) torch.Size([3, 2592])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    src_tokens[\"value_tokens\"].shape,\n",
    "    src_tokens[\"coord_type_tokens\"].shape,\n",
    "    src_tokens[\"position_tokens\"].shape,\n",
    "    src_tokens[\"padding_mask\"].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 864, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb = src_embed.forward_original(src_tokens)\n",
    "src_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 864, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb = src_embed(src_tokens)\n",
    "src_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value_tokens': tensor([[  0, 206, 205,  ...,   2,   2,   2],\n",
       "         [  0,  64,  63,  ...,   2,   2,   2],\n",
       "         [  0,  66,  66,  ...,   2,   2,   2]]),\n",
       " 'target_tokens': tensor([[206, 205, 203,  ...,   2,   2,   2],\n",
       "         [ 64,  63,  61,  ...,   2,   2,   2],\n",
       "         [ 66,  66,  64,  ...,   2,   2,   2]]),\n",
       " 'in_position_tokens': tensor([[1, 2, 3,  ..., 0, 0, 0],\n",
       "         [1, 2, 3,  ..., 0, 0, 0],\n",
       "         [1, 2, 3,  ..., 0, 0, 0]]),\n",
       " 'out_position_tokens': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'ref_v_mask': tensor([[0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " 'ref_v_ids': tensor([[  0, 203, 202,  ...,   0,   0,   0],\n",
       "         [  0,  61,  60,  ...,   0,   0,   0],\n",
       "         [  0,  63,  63,  ...,   0,   0,   0]]),\n",
       " 'ref_e_mask': tensor([[1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [1., 0., 0.,  ..., 1., 1., 1.]]),\n",
       " 'ref_e_ids': tensor([[0, 0, 0,  ..., 2, 2, 2],\n",
       "         [0, 0, 0,  ..., 2, 2, 2],\n",
       "         [0, 0, 0,  ..., 2, 2, 2]]),\n",
       " 'padding_mask': tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokens = dec_tokenizer.tokenize(f_batch)\n",
    "tgt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDecoderEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim=256,\n",
    "                 vocab_value=3, pad_idx_value=2, \n",
    "                 vocab_in_position=100, pad_idx_in_position=0,\n",
    "                 vocab_out_position=1000, pad_idx_out_position=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.value_embed = nn.Embedding(\n",
    "            vocab_value, embed_dim, padding_idx=pad_idx_value\n",
    "        )\n",
    "        self.in_position_embed = nn.Embedding(\n",
    "            vocab_in_position, embed_dim, padding_idx=pad_idx_in_position\n",
    "        )\n",
    "        self.out_position_embed = nn.Embedding(\n",
    "            vocab_out_position, embed_dim, padding_idx=pad_idx_out_position\n",
    "        )\n",
    "        \n",
    "        self.embed_scaler = math.sqrt(embed_dim)\n",
    "        \n",
    "    def forward(self, encoder_embed, tokens):\n",
    "        \n",
    "        \"\"\"get embedding for Face Decoder.\n",
    "        note that value_embeddings consist of two embedding.\n",
    "          - pointer to encoder outputs\n",
    "          - embedding for special tokens such as <end-of-face>, <eos>, <pad>.\n",
    "        \n",
    "        Args\n",
    "            encoder_embed [torch.tensor]:\n",
    "                    (batch, src-length, embed) shape tensor from encoder.\n",
    "            tokens [dict]: all contents are in the shape of (batch, tgt-length).\n",
    "                `ref_v_ids` [torch.tensor]:\n",
    "                        this is used as pointer to `encoder_embed`.\n",
    "                `ref_v_mask` [torch.tensor]:\n",
    "                        mask for special token positions in pointer embeddings. \n",
    "                `ref_e_ids` [torch.tensor]:\n",
    "                        embed ids for special tokens.\n",
    "                `ref_e_ids` [torch.tensor]:\n",
    "                        mask for pointer token position in special token embeddings.\n",
    "                `in_position_tokens` [torch.tensor]:\n",
    "                        embed ids for positions in face.\n",
    "                `out_position_tokens` [torch.tensor]:\n",
    "                        embed ids for positions of face itself in sequence.\n",
    "                        \n",
    "        Returns\n",
    "            embed [torch.tensor]: (batch, tgt-length, embed) shape tensor of embeddings.\n",
    "                        \n",
    "        \"\"\"\n",
    "        \n",
    "        embed = torch.cat([\n",
    "            encoder_embed[b_idx, ids].unsqueeze(dim=0) \n",
    "            for b_idx, ids in enumerate(tokens[\"ref_v_ids\"].unbind(dim=0))\n",
    "        ], dim=0)\n",
    "        embed = embed * tokens[\"ref_v_mask\"].unsqueeze(dim=2)\n",
    "        \n",
    "        embed = (embed + \\\n",
    "                (self.value_embed(tokens[\"ref_e_ids\"]) * \n",
    "                 self.embed_scaler *\n",
    "                 tokens[\"ref_e_mask\"].unsqueeze(dim=2)))\n",
    "        \n",
    "        embed = embed + (self.in_position_embed(tokens[\"in_position_tokens\"]) * self.embed_scaler)\n",
    "        embed = embed + (self.out_position_embed(tokens[\"out_position_tokens\"]) * self.embed_scaler)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_embed = FaceDecoderEmbedding(embed_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3936, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_emb = tgt_embed(src_emb, tgt_tokens)\n",
    "tgt_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3936, 864])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(tgt_emb, src_emb.permute(0, 2, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_embed.value_embed.weight[None, ...].repeat(3, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 867, 128])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "BATCH = 3\n",
    "test = torch.cat([\n",
    "    tgt_embed.value_embed.weight[None, ...].repeat(BATCH, 1, 1),\n",
    "    src_emb\n",
    "], dim=1)\n",
    "print(test.shape)\n",
    "print(test[:, 2]) # id 2 is padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3936, 867])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(tgt_emb, test.permute(0, 2, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \n",
    "    def write_to_json(self, out_path):\n",
    "        with open(out_path, \"w\") as fw:\n",
    "            json.dump(self.config, fw, indent=4)\n",
    "            \n",
    "    def load_from_json(self, file_path):\n",
    "        with open(file_path) as fr:\n",
    "            self.config = json.load(fr)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self.config[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacePolyGenConfig(Config):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_dim=256, \n",
    "                 src__max_seq_len=2400, \n",
    "                 src__tokenizer__pad_id=0,\n",
    "                 tgt__max_seq_len=3900,\n",
    "                 tgt__tokenizer__bof_id=0,\n",
    "                 tgt__tokenizer__eos_id=1, \n",
    "                 tgt__tokenizer__pad_id=2,\n",
    "                 src__embedding__vocab_value=256 + 3, \n",
    "                 src__embedding__vocab_coord_type=4, \n",
    "                 src__embedding__vocab_position=1000, \n",
    "                 src__embedding__pad_idx_value=2,\n",
    "                 src__embedding__pad_idx_coord_type=0,\n",
    "                 src__embedding__pad_idx_position=0,\n",
    "                 tgt__embedding__vocab_value=3,\n",
    "                 tgt__embedding__vocab_in_position=100,\n",
    "                 tgt__embedding__vocab_out_position=1000,\n",
    "                 tgt__embedding__pad_idx_value=2,\n",
    "                 tgt__embedding__pad_idx_in_position=0,\n",
    "                 tgt__embedding__pad_idx_out_position=0,\n",
    "                 src__reformer__depth=12,\n",
    "                 src__reformer__heads=8,\n",
    "                 src__reformer__n_hashes=8,\n",
    "                 src__reformer__bucket_size=48,\n",
    "                 src__reformer__causal=True,\n",
    "                 src__reformer__lsh_dropout=0.2, \n",
    "                 src__reformer__ff_dropout=0.2,\n",
    "                 src__reformer__post_attn_dropout=0.2,\n",
    "                 src__reformer__ff_mult=4,\n",
    "                 tgt__reformer__depth=12,\n",
    "                 tgt__reformer__heads=8,\n",
    "                 tgt__reformer__n_hashes=8,\n",
    "                 tgt__reformer__bucket_size=48,\n",
    "                 tgt__reformer__causal=True,\n",
    "                 tgt__reformer__lsh_dropout=0.2, \n",
    "                 tgt__reformer__ff_dropout=0.2,\n",
    "                 tgt__reformer__post_attn_dropout=0.2,\n",
    "                 tgt__reformer__ff_mult=4):\n",
    "        \n",
    "        # auto padding for max_seq_len\n",
    "        src_denominator = (src__reformer__bucket_size * 2 * 3)\n",
    "        if src__max_seq_len % src_denominator != 0:\n",
    "            divisables = src__max_seq_len // src_denominator + 1\n",
    "            src__max_seq_len_new = divisables * src_denominator\n",
    "            print(\"src__max_seq_len changed, because of lsh-attention's bucket_size\")\n",
    "            print(\"before: {} --> after: {} (with bucket_size: {})\".format(\n",
    "                src__max_seq_len, src__max_seq_len_new, src__reformer__bucket_size\n",
    "            ))\n",
    "            src__max_seq_len = src__max_seq_len_new\n",
    "            \n",
    "        tgt_denominator = tgt__reformer__bucket_size * 2\n",
    "        if tgt__max_seq_len % tgt_denominator != 0:\n",
    "            divisables = tgt__max_seq_len // tgt_denominator + 1\n",
    "            tgt__max_seq_len_new = divisables * tgt_denominator\n",
    "            print(\"tgt__max_seq_len changed, because of lsh-attention's bucket_size\")\n",
    "            print(\"before: {} --> after: {} (with bucket_size: {})\".format(\n",
    "                tgt__max_seq_len, tgt__max_seq_len_new, tgt__reformer__bucket_size\n",
    "            ))\n",
    "            tgt__max_seq_len = tgt__max_seq_len_new\n",
    "        \n",
    "        \n",
    "        # tokenizer config\n",
    "        src_tokenizer_config = {\n",
    "            \"pad_id\": src__tokenizer__pad_id,\n",
    "            \"max_seq_len\": src__max_seq_len,\n",
    "        }\n",
    "        tgt_tokenizer_config = {\n",
    "            \"bof_id\": tgt__tokenizer__bof_id,\n",
    "            \"eos_id\": tgt__tokenizer__eos_id,\n",
    "            \"pad_id\": tgt__tokenizer__pad_id,\n",
    "            \"max_seq_len\": tgt__max_seq_len,\n",
    "        }\n",
    "        \n",
    "        # embedding config\n",
    "        src_embedding_config = {\n",
    "            \"vocab_value\": src__embedding__vocab_value,\n",
    "            \"vocab_coord_type\": src__embedding__vocab_coord_type,\n",
    "            \"vocab_position\": src__embedding__vocab_position,\n",
    "            \"pad_idx_value\": src__embedding__pad_idx_value,\n",
    "            \"pad_idx_coord_type\": src__embedding__pad_idx_coord_type,\n",
    "            \"pad_idx_position\": src__embedding__pad_idx_position,\n",
    "            \"embed_dim\": embed_dim,\n",
    "        }\n",
    "        tgt_embedding_config = {\n",
    "            \"vocab_value\": tgt__embedding__vocab_value,\n",
    "            \"vocab_in_position\": tgt__embedding__vocab_in_position,\n",
    "            \"vocab_out_position\": tgt__embedding__vocab_out_position,\n",
    "            \"pad_idx_value\": tgt__embedding__pad_idx_value,\n",
    "            \"pad_idx_in_position\": tgt__embedding__pad_idx_in_position,\n",
    "            \"pad_idx_out_position\": tgt__embedding__pad_idx_out_position,\n",
    "            \"embed_dim\": embed_dim,\n",
    "        }\n",
    "        \n",
    "        # reformer info\n",
    "        src_reformer_config = {\n",
    "            \"dim\": embed_dim,\n",
    "            \"max_seq_len\": src__max_seq_len,\n",
    "            \"depth\": src__reformer__depth,\n",
    "            \"heads\": src__reformer__heads,\n",
    "            \"bucket_size\": src__reformer__bucket_size,\n",
    "            \"n_hashes\": src__reformer__n_hashes,\n",
    "            \"causal\": src__reformer__causal,\n",
    "            \"lsh_dropout\": src__reformer__lsh_dropout, \n",
    "            \"ff_dropout\": src__reformer__ff_dropout,\n",
    "            \"post_attn_dropout\": src__reformer__post_attn_dropout,\n",
    "            \"ff_mult\": src__reformer__ff_mult,\n",
    "        }\n",
    "        \n",
    "        tgt_reformer_config = {\n",
    "            \"dim\": embed_dim,\n",
    "            \"max_seq_len\": tgt__max_seq_len,\n",
    "            \"depth\": tgt__reformer__depth,\n",
    "            \"heads\": tgt__reformer__heads,\n",
    "            \"bucket_size\": tgt__reformer__bucket_size,\n",
    "            \"n_hashes\": tgt__reformer__n_hashes,\n",
    "            \"causal\": tgt__reformer__causal,\n",
    "            \"lsh_dropout\": tgt__reformer__lsh_dropout, \n",
    "            \"ff_dropout\": tgt__reformer__ff_dropout,\n",
    "            \"post_attn_dropout\": tgt__reformer__post_attn_dropout,\n",
    "            \"ff_mult\": tgt__reformer__ff_mult,\n",
    "        }\n",
    "        \n",
    "        self.config = {\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"src_tokenizer\": src_tokenizer_config,\n",
    "            \"tgt_tokenizer\": tgt_tokenizer_config,\n",
    "            \"src_embedding\": src_embedding_config,\n",
    "            \"tgt_embedding\": tgt_embedding_config,\n",
    "            \"src_reformer\": src_reformer_config,\n",
    "            \"tgt_reformer\": tgt_reformer_config,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def accuracy(y_pred, y_true, ignore_label=None, device=None):\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "\n",
    "    if ignore_label:\n",
    "        normalizer = torch.sum(y_true!=ignore_label)\n",
    "        ignore_mask = torch.where(\n",
    "            y_true == ignore_label,\n",
    "            torch.zeros_like(y_true, device=device),\n",
    "            torch.ones_like(y_true, device=device)\n",
    "        ).type(torch.float32)\n",
    "    else:\n",
    "        normalizer = y_true.shape[0]\n",
    "        ignore_mask = torch.ones_like(y_true, device=device).type(torch.float32)\n",
    "\n",
    "    acc = (y_pred.reshape(-1)==y_true.reshape(-1)).type(torch.float32)\n",
    "    acc = torch.sum(acc*ignore_mask)\n",
    "    return acc / normalizer\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "    if type(m) == nn.Embedding:\n",
    "        nn.init.uniform_(m.weight, -0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacePolyGen(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        self.src_tokenizer = EncodeVertexTokenizer(**model_config[\"src_tokenizer\"])\n",
    "        self.tgt_tokenizer = FaceTokenizer(**model_config[\"tgt_tokenizer\"])\n",
    "        \n",
    "        self.src_embedding = FaceEncoderEmbedding(**model_config[\"src_embedding\"])\n",
    "        self.tgt_embedding = FaceDecoderEmbedding(**model_config[\"tgt_embedding\"])\n",
    "        \n",
    "        self.src_reformer = Reformer(**model_config[\"src_reformer\"])\n",
    "        self.tgt_reformer = Reformer(**model_config[\"tgt_reformer\"])\n",
    "        \n",
    "        self.src_norm = nn.LayerNorm(model_config[\"embed_dim\"])\n",
    "        self.tgt_norm = nn.LayerNorm(model_config[\"embed_dim\"])\n",
    "        self.loss_func = nn.CrossEntropyLoss(ignore_index=model_config[\"tgt_tokenizer\"][\"pad_id\"])\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        self.embed_scaler = math.sqrt(model_config[\"embed_dim\"])\n",
    "    \n",
    "    def forward(self, src_tokens, device=None):\n",
    "        \n",
    "        \"\"\"forward function which can be used for both train/predict.\n",
    "        this function only encodes vertex information\n",
    "        because decoders behave as really auto-regressive function.\n",
    "        \n",
    "        Args\n",
    "            src_tokens [dict]: tokenized vertex info.\n",
    "                `value_tokens` [torch.tensor]:\n",
    "                        padded (batch, src-length) shape long tensor\n",
    "                        with coord value from 0 to 2^n(bit).\n",
    "                `coord_type_tokens` [torch.tensor]:\n",
    "                        padded (batch, src-length) shape long tensor implies x or y or z.\n",
    "                `position_tokens` [torch.tensor]:\n",
    "                        padded (batch, src-length) shape long tensor\n",
    "                        representing coord position (NOT sequence position).\n",
    "                `padding_mask` [torch.tensor]:\n",
    "                        (batch, src-length) shape mask implies <pad> tokens.\n",
    "        \n",
    "        Returns\n",
    "            hs [torch.tensor]: (batch, src-length, embed) shape tensor after encoder.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        hs = self.src_embedding(src_tokens)\n",
    "        hs = self.src_reformer(\n",
    "            hs, input_mask=src_tokens[\"padding_mask\"]\n",
    "        )\n",
    "        hs = self.src_norm(hs)\n",
    "        \n",
    "        return hs\n",
    "        \n",
    "    def __call__(self, inputs, device=None):\n",
    "        \n",
    "        \"\"\"Calculate loss while training.\n",
    "        \n",
    "        Args\n",
    "            inputs [dict]: dict containing batched inputs.\n",
    "                `vertices` [list(torch.tensor)]:\n",
    "                        variable-length-list of \n",
    "                        (length, 3) shaped tensor of quantized-vertices.\n",
    "                `faces` [list(list(torch.tensor))]:\n",
    "                        batch-length-list of\n",
    "                        variable-length-list (per face) of \n",
    "                        (length,) shaped vertex-ids which constructs a face.\n",
    "            device [torch.device]: gpu or not gpu, that's the problem.\n",
    "                \n",
    "        Returns\n",
    "            outputs [dict]: dict containing calculated variables.\n",
    "                `loss` [torch.tensor]:\n",
    "                        calculated scalar-shape loss with backprop info.\n",
    "                `accuracy` [torch.tensor]:\n",
    "                        calculated scalar-shape accuracy.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        src_tokens = self.src_tokenizer.tokenize(inputs[\"vertices\"])\n",
    "        src_tokens = {k: v.to(device) for k, v in src_tokens.items()}\n",
    "        \n",
    "        tgt_tokens = self.tgt_tokenizer.tokenize(inputs[\"faces\"])\n",
    "        tgt_tokens = {k: v.to(device) for k, v in tgt_tokens.items()}\n",
    "        \n",
    "        encoder_embed = self.forward(src_tokens, device=device)\n",
    "        hs = self.tgt_embedding(encoder_embed, tgt_tokens)\n",
    "        hs = self.tgt_reformer(\n",
    "            hs, input_mask=tgt_tokens[\"padding_mask\"]\n",
    "        )\n",
    "        hs = self.tgt_norm(hs)\n",
    "        \n",
    "        # calc pointing to vertex\n",
    "        BATCH = hs.shape[0]\n",
    "        sptk_embed = self.tgt_embedding.value_embed.weight\n",
    "        encoder_embed = torch.cat([\n",
    "            sptk_embed[None, ...].repeat(BATCH, 1, 1),\n",
    "            encoder_embed\n",
    "        ], dim=1)\n",
    "        hs = torch.bmm(hs, encoder_embed.permute(0, 2, 1))\n",
    "        \n",
    "        BATCH, TGT_LENGTH, SRC_LENGTH = hs.shape\n",
    "        hs = hs.reshape(BATCH*TGT_LENGTH, SRC_LENGTH)\n",
    "        targets = tgt_tokens[\"target_tokens\"].reshape(BATCH*TGT_LENGTH,)\n",
    "        \n",
    "        acc = accuracy(\n",
    "            hs, targets, ignore_label=self.tgt_tokenizer.pad_id, device=device\n",
    "        )\n",
    "        loss = self.loss_func(hs, targets)\n",
    "        \n",
    "        outputs = {\n",
    "            \"accuracy\": acc,\n",
    "            \"perplexity\": torch.exp(loss),\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "        return outputs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, inputs, max_seq_len=3936, device=None):\n",
    "        tgt_tokenizer = self.tgt_tokenizer\n",
    "        special_tokens = tgt_tokenizer.special_tokens\n",
    "        \n",
    "        # calc vertex encoding first.\n",
    "        src_tokens = self.src_tokenizer.tokenize(inputs[\"vertices\"])\n",
    "        src_tokens = {k: v.to(device) for k, v in src_tokens.items()}\n",
    "        \n",
    "        encoder_embed = self.forward(src_tokens, device=device)\n",
    "        BATCH = encoder_embed.shape[0]\n",
    "        sptk_embed = self.tgt_embedding.value_embed.weight\n",
    "        encoder_embed_with_sptk = torch.cat([\n",
    "            sptk_embed[None, ...].repeat(BATCH, 1, 1),\n",
    "            encoder_embed\n",
    "        ], dim=1)\n",
    "        \n",
    "        # prepare for generation.\n",
    "        tgt_tokens = model.tgt_tokenizer.tokenize([[torch.tensor([], dtype=torch.int32)]])\n",
    "        tgt_tokens[\"value_tokens\"][:, 1] = model.tgt_tokenizer.special_tokens[\"pad\"]\n",
    "        tgt_tokens[\"ref_e_ids\"][:, 1] = model.tgt_tokenizer.special_tokens[\"pad\"]\n",
    "        tgt_tokens[\"padding_mask\"][:, 1] = True\n",
    "        \n",
    "        preds = [torch.tensor([], dtype=torch.int32)]\n",
    "        pred_idx = 0\n",
    "        \n",
    "        now_face_idx = 0\n",
    "        \n",
    "        while (pred_idx <= max_seq_len-1) \\\n",
    "        and ((len(preds[now_face_idx]) == 0)\n",
    "            or (preds[now_face_idx][-1] != special_tokens[\"eos\"])):\n",
    "            \n",
    "            if pred_idx >= 1:\n",
    "                tgt_tokens = tgt_tokenizer.tokenize([[torch.cat([p]) for p in preds]])\n",
    "                tgt_tokens[\"value_tokens\"][:, pred_idx+1] = special_tokens[\"pad\"]\n",
    "                tgt_tokens[\"ref_e_ids\"][:, pred_idx+1] = special_tokens[\"pad\"]\n",
    "                tgt_tokens[\"padding_mask\"][:, pred_idx+1] = True\n",
    "            \n",
    "            try:\n",
    "                hs = self.tgt_embedding(encoder_embed, tgt_tokens)\n",
    "            except IndexError:\n",
    "                print(\"pred_ids\", pred_idx)\n",
    "                for k, v in tgt_tokens.items():\n",
    "                    print(k)\n",
    "                    print(v.shape)\n",
    "                    print(v[:, pred_idx-5:pred_idx+2])\n",
    "                print(preds)\n",
    "                raise IndexError\n",
    "                \n",
    "            hs = self.tgt_reformer(\n",
    "                hs, input_mask=tgt_tokens[\"padding_mask\"]\n",
    "            )\n",
    "            hs = self.tgt_norm(hs)\n",
    "            \n",
    "            hs = torch.bmm(\n",
    "                hs[:, pred_idx:pred_idx+1],\n",
    "                encoder_embed_with_sptk.permute(0, 2, 1)\n",
    "            )\n",
    "            pred = hs[:, 0].argmax(dim=1)\n",
    "            \n",
    "            if pred[0] == special_tokens[\"bof\"]:\n",
    "                now_face_idx += 1\n",
    "                preds.append(torch.tensor([], dtype=torch.int32))\n",
    "            else:\n",
    "                preds[now_face_idx] = \\\n",
    "                    torch.cat([preds[now_face_idx], pred[0, None]-len(special_tokens)])\n",
    "            pred_idx += 1\n",
    "            \n",
    "        preds = torch.cat(preds) + len(special_tokens)\n",
    "        preds = self.tgt_tokenizer.detokenize([preds])[0]    \n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertices\n",
      "torch.Size([204, 3])\n",
      "torch.Size([62, 3])\n",
      "faces\n",
      "160\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"vertices\": v_batch[:2],\n",
    "    \"faces\": f_batch[:2],\n",
    "}\n",
    "for k, values in inputs.items():\n",
    "    print(k)\n",
    "    for v in values:\n",
    "        if k == \"vertices\":\n",
    "            print(v.shape)\n",
    "        else:\n",
    "            print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src__max_seq_len changed, because of lsh-attention's bucket_size\n",
      "before: 2400 --> after: 2592 (with bucket_size: 48)\n",
      "tgt__max_seq_len changed, because of lsh-attention's bucket_size\n",
      "before: 3900 --> after: 3936 (with bucket_size: 48)\n"
     ]
    }
   ],
   "source": [
    "config = FacePolyGenConfig(\n",
    "    embed_dim=64, \n",
    "    src__reformer__depth=4, src__reformer__lsh_dropout=0.,\n",
    "    src__reformer__ff_dropout=0., src__reformer__post_attn_dropout=0.,\n",
    "    tgt__reformer__depth=4, tgt__reformer__lsh_dropout=0.,\n",
    "    tgt__reformer__ff_dropout=0., tgt__reformer__post_attn_dropout=0.\n",
    ")\n",
    "model = FacePolyGen(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\tloss: 29.97350\tperp: 10406975242240.000\tacc: 0.01789\n",
      "iteration: 10\tloss: 16.18116\tperp: 27518683256.656\tacc: 0.01261\n",
      "iteration: 20\tloss: 6.63694\tperp: 1450.166\tacc: 0.09851\n",
      "iteration: 30\tloss: 5.30849\tperp: 205.229\tacc: 0.17815\n",
      "iteration: 40\tloss: 4.97836\tperp: 145.751\tacc: 0.19264\n",
      "iteration: 50\tloss: 4.77371\tperp: 118.577\tacc: 0.20351\n",
      "iteration: 60\tloss: 4.56591\tperp: 96.348\tacc: 0.21404\n",
      "iteration: 70\tloss: 4.31224\tperp: 74.851\tacc: 0.22527\n",
      "iteration: 80\tloss: 4.00551\tperp: 55.138\tacc: 0.24145\n",
      "iteration: 90\tloss: 3.67401\tperp: 39.591\tacc: 0.26747\n",
      "iteration: 100\tloss: 3.34354\tperp: 28.443\tacc: 0.30353\n",
      "iteration: 110\tloss: 3.03268\tperp: 20.827\tacc: 0.33625\n",
      "iteration: 120\tloss: 2.75769\tperp: 15.806\tacc: 0.36251\n",
      "iteration: 130\tloss: 2.51812\tperp: 12.431\tacc: 0.38141\n",
      "iteration: 140\tloss: 2.30530\tperp: 10.044\tacc: 0.39205\n",
      "iteration: 150\tloss: 2.10852\tperp: 8.249\tacc: 0.40410\n",
      "iteration: 160\tloss: 1.91848\tperp: 6.821\tacc: 0.42101\n",
      "iteration: 170\tloss: 1.72774\tperp: 5.637\tacc: 0.44834\n",
      "iteration: 180\tloss: 1.53123\tperp: 4.632\tacc: 0.49257\n",
      "iteration: 190\tloss: 1.32772\tperp: 3.779\tacc: 0.55558\n",
      "iteration: 200\tloss: 1.12100\tperp: 3.073\tacc: 0.63065\n",
      "iteration: 210\tloss: 0.91669\tperp: 2.505\tacc: 0.70940\n",
      "iteration: 220\tloss: 0.72248\tperp: 2.062\tacc: 0.78648\n",
      "iteration: 230\tloss: 0.54885\tperp: 1.733\tacc: 0.86138\n",
      "iteration: 240\tloss: 0.40422\tperp: 1.499\tacc: 0.92253\n",
      "iteration: 250\tloss: 0.29235\tperp: 1.340\tacc: 0.96375\n",
      "iteration: 260\tloss: 0.21089\tperp: 1.235\tacc: 0.98322\n",
      "iteration: 270\tloss: 0.15407\tperp: 1.167\tacc: 0.99300\n",
      "iteration: 280\tloss: 0.11532\tperp: 1.122\tacc: 0.99758\n",
      "iteration: 290\tloss: 0.08904\tperp: 1.093\tacc: 0.99950\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "epoch_num = 300\n",
    "model.train()\n",
    "losses = []\n",
    "accs = []\n",
    "perps = []\n",
    "\n",
    "for i in range(epoch_num):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    loss = outputs[\"loss\"]\n",
    "    acc = outputs[\"accuracy\"]\n",
    "    perp = outputs[\"perplexity\"]\n",
    "    losses.append(loss.item())\n",
    "    accs.append(acc.item())\n",
    "    perps.append(perp.item())\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        ave_loss = np.mean(losses[-10:])\n",
    "        ave_acc = np.mean(accs[-10:])\n",
    "        ave_perp = np.mean(perps[-10:])\n",
    "        print(\"iteration: {}\\tloss: {:.5f}\\tperp: {:.3f}\\tacc: {:.5f}\".format(\n",
    "            i, ave_loss, ave_perp, ave_acc))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"../results/models/face3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_ids 99\n",
      "value_tokens\n",
      "torch.Size([1, 3936])\n",
      "tensor([[566, 566, 566, 566, 566, 566,   2]])\n",
      "target_tokens\n",
      "torch.Size([1, 3936])\n",
      "tensor([[566, 566, 566, 566, 566,   1,   2]])\n",
      "in_position_tokens\n",
      "torch.Size([1, 3936])\n",
      "tensor([[ 95,  96,  97,  98,  99, 100,   0]])\n",
      "out_position_tokens\n",
      "torch.Size([1, 3936])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 0]])\n",
      "ref_v_mask\n",
      "torch.Size([1, 3936])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 0.]])\n",
      "ref_v_ids\n",
      "torch.Size([1, 3936])\n",
      "tensor([[563, 563, 563, 563, 563, 563,   0]])\n",
      "ref_e_mask\n",
      "torch.Size([1, 3936])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1.]])\n",
      "ref_e_ids\n",
      "torch.Size([1, 3936])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 2]])\n",
      "padding_mask\n",
      "torch.Size([1, 3936])\n",
      "tensor([[False, False, False, False, False, False,  True]])\n",
      "[tensor([563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563,\n",
      "        563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563,\n",
      "        563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563,\n",
      "        563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563,\n",
      "        563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563,\n",
      "        563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563,\n",
      "        563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563, 563,\n",
      "        563])]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-40a0dc0d5253>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs, max_seq_len, device)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/3dEnv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-14-962a9b1dcb66>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_embed, tokens)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_position_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"in_position_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_position_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"out_position_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/3dEnv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/3dEnv/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/3dEnv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-7f23a740d30b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m pred = model.predict(\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"vertices\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vertices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_in_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/3dEnv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-40a0dc0d5253>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs, max_seq_len, device)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_idx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             hs = self.tgt_reformer(\n",
      "\u001b[0;31mIndexError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx_in_batch = 0\n",
    "model.load_state_dict(torch.load(\"../results/models/face3\"))\n",
    "model.eval()\n",
    "pred = model.predict(\n",
    "    {\"vertices\": [inputs[\"vertices\"][idx_in_batch]]},\n",
    "    max_seq_len=101,\n",
    ")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = torch.cat(inputs[\"faces\"][idx_in_batch])\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(pred.shape[0], true.shape[0])\n",
    "acc = (pred[:min_len] == true[:min_len]).sum() / min_len\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_in_batch = 1\n",
    "model.load_state_dict(torch.load(\"../results/models/face3\"))\n",
    "model.eval()\n",
    "pred = model.predict(\n",
    "    {\"vertices\": [inputs[\"vertices\"][idx_in_batch]]},\n",
    ")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = torch.cat(inputs[\"faces\"][idx_in_batch])\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(pred.shape[0], true.shape[0])\n",
    "acc = (pred[:min_len] == true[:min_len]).sum() / min_len\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
