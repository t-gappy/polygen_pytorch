{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7003 1088\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(base_dir, \"data\", \"original\")\n",
    "train_files = glob.glob(os.path.join(data_dir, \"train\", \"*\", \"*.obj\"))\n",
    "valid_files = glob.glob(os.path.join(data_dir, \"val\", \"*\", \"*.obj\"))\n",
    "print(len(train_files), len(valid_files))\n",
    "\n",
    "src_dir = os.path.join(base_dir, \"src\")\n",
    "sys.path.append(os.path.join(src_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_pipeline\n",
    "from tokenizers import EncodeVertexTokenizer, FaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([655, 3]) torch.Size([1317, 3])\n",
      "============================================================\n",
      "torch.Size([310, 3]) torch.Size([676, 3])\n",
      "============================================================\n",
      "torch.Size([396, 3]) torch.Size([1184, 3])\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "v_batch, f_batch = [], []\n",
    "for i in range(3):\n",
    "    v, f = load_pipeline(train_files[i])\n",
    "    v = torch.tensor(v)\n",
    "    f = torch.tensor(f)\n",
    "    v_batch.append(v)\n",
    "    f_batch.append(f)\n",
    "    print(v.shape, f.shape)\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer = EncodeVertexTokenizer()\n",
    "dec_tokenizer = FaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value_tokens': tensor([[161, 137, 133,  ..., 137, 123,   0],\n",
       "         [135, 162, 130,  ...,   0,   0,   0],\n",
       "         [163,  99, 134,  ...,   0,   0,   0]]),\n",
       " 'coord_type_tokens': tensor([[1, 2, 3,  ..., 2, 3, 0],\n",
       "         [1, 2, 3,  ..., 0, 0, 0],\n",
       "         [1, 2, 3,  ..., 0, 0, 0]]),\n",
       " 'position_ids': tensor([[  1,   1,   1,  ..., 655, 655,   0],\n",
       "         [  1,   1,   1,  ...,   0,   0,   0],\n",
       "         [  1,   1,   1,  ...,   0,   0,   0]]),\n",
       " 'padding_mask': tensor([[False, False, False,  ..., False, False,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tokenizer.tokenize(v_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value_tokens': tensor([[  3,   4, 156,  ...,   0,   1,   2],\n",
       "         [  3,   5,  29,  ...,   2,   2,   2],\n",
       "         [  3,   3,   4,  ...,   2,   2,   2]]),\n",
       " 'coord_type_tokens': tensor([[1, 2, 3,  ..., 4, 0, 0],\n",
       "         [1, 2, 3,  ..., 0, 0, 0],\n",
       "         [1, 2, 3,  ..., 0, 0, 0]]),\n",
       " 'position_ids': tensor([[   1,    1,    1,  ..., 1317,    0,    0],\n",
       "         [   1,    1,    1,  ...,    0,    0,    0],\n",
       "         [   1,    1,    1,  ...,    0,    0,    0]]),\n",
       " 'ref_v_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " 'ref_v_ids': tensor([[  0,   1, 153,  ...,   0,   0,   0],\n",
       "         [  0,   2,  26,  ...,   0,   0,   0],\n",
       "         [  0,   0,   1,  ...,   0,   0,   0]]),\n",
       " 'ref_e_mask': tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.]]),\n",
       " 'ref_e_ids': tensor([[0, 0, 0,  ..., 0, 1, 2],\n",
       "         [0, 0, 0,  ..., 2, 2, 2],\n",
       "         [0, 0, 0,  ..., 2, 2, 2]]),\n",
       " 'padding_mask': tensor([[False, False, False,  ..., False, False,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True]]),\n",
       " 'future_mask': tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "         [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tokenizer.tokenize(f_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dEnv",
   "language": "python",
   "name": "3denv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
